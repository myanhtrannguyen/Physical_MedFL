{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7040769,"sourceType":"datasetVersion","datasetId":4020939},{"sourceId":8221096,"sourceType":"datasetVersion","datasetId":4873800}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MLP","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport time\nimport os\nimport h5py\nfrom skimage.transform import resize\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:12.922946Z","iopub.execute_input":"2025-08-04T10:04:12.923177Z","iopub.status.idle":"2025-08-04T10:04:18.017297Z","shell.execute_reply.started":"2025-08-04T10:04:12.923152Z","shell.execute_reply":"2025-08-04T10:04:18.016717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration ---\nNUM_EPOCHS_CENTRALIZED = 50\nNUM_CLASSES = 4\nLEARNING_RATE = 1e-5\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nIMG_SIZE = 256\nBATCH_SIZE = 8 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:18.019164Z","iopub.execute_input":"2025-08-04T10:04:18.019626Z","iopub.status.idle":"2025-08-04T10:04:18.085129Z","shell.execute_reply.started":"2025-08-04T10:04:18.019605Z","shell.execute_reply":"2025-08-04T10:04:18.084437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Standard Convolutional Block ---\nclass BasicConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, use_bn=True):\n        super().__init__()\n        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=not use_bn)]\n        if use_bn:\n            layers.append(nn.BatchNorm2d(out_channels))\n        layers.append(nn.ReLU(inplace=True))\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:18.085985Z","iopub.execute_input":"2025-08-04T10:04:18.086261Z","iopub.status.idle":"2025-08-04T10:04:18.100897Z","shell.execute_reply.started":"2025-08-04T10:04:18.086234Z","shell.execute_reply":"2025-08-04T10:04:18.100242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ePURE","metadata":{}},{"cell_type":"code","source":"# --- ePURE Implementation (Provided) ---\nclass ePURE(nn.Module):\n    def __init__(self, in_channels, base_channels=32):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(base_channels, 1, 3, padding=1) # Ensure output is 1 channel for noise profile\n        )\n\n    def forward(self, x):\n        x_float = x.float()\n\n        # Estimate a base noise map from the input features\n        noise_map_raw = self.conv(x_float) # Output is [B, 1, H, W]\n\n        # Simple approach: just output the learned map directly.\n        # The adaptive smoothing uses sigmoid, so the network learns to output values\n        # that sigmoid can map to appropriate blending weights.\n        noise_map = noise_map_raw # [B, 1, H, W]\n\n        return noise_map # Noise profile estimate (1 channel)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:18.101599Z","iopub.execute_input":"2025-08-04T10:04:18.101823Z","iopub.status.idle":"2025-08-04T10:04:18.114881Z","shell.execute_reply.started":"2025-08-04T10:04:18.101806Z","shell.execute_reply":"2025-08-04T10:04:18.114251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adaptive_Spline_Function","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms.functional as TF\n# --- Adaptive Spline Smoothing Implementation (Provided) ---\ndef adaptive_spline_smoothing(x, noise_profile, kernel_size=5, sigma=1.0):\n    \"\"\"\n    Áp dụng làm mịn thích nghi dựa trên noise_profile\n    - x: Ảnh đầu vào hoặc feature map [B, C, H, W]\n    - noise_profile: Bản đồ nhiễu [B, 1, H, W] (giá trị từ 0 đến 1)\n    - kernel_size/sigma: Tham số làm mịn Gaussian\n    \"\"\"\n    # Ensure input is float for convolution\n    x_float = x.float()\n\n    # Ensure noise_profile is float and 1 channel\n    noise_profile_float = noise_profile.float()\n    if noise_profile_float.size(1) != 1:\n         print(f\"Warning: Noise profile expected 1 channel but got {noise_profile_float.size(1)}. Using first channel.\")\n         noise_profile_float = noise_profile_float[:, :1, :, :]\n\n\n    # Bước 1: Làm mịn ảnh bằng Gaussian blur\n    # Apply Gaussian blur channel-wise\n    # kernel_size can be a single int or a tuple (h, w). sigma same.\n    # Ensure kernel_size is a tuple if needed, or check F.gaussian_blur docs.\n    # F.gaussian_blur expects kernel_size as a tuple of ints (h, w).\n    # If kernel_size is an int, it uses that for both dims.\n    if isinstance(kernel_size, int):\n        kernel_size_tuple = (kernel_size, kernel_size)\n    else:\n        kernel_size_tuple = kernel_size\n\n    if isinstance(sigma, (int, float)):\n         sigma_tuple = (float(sigma), float(sigma))\n    else:\n         sigma_tuple = sigma\n\n    # Ensure sigma values are positive to avoid issues\n    sigma_tuple = tuple(max(0.1, s) for s in sigma_tuple) # Add small epsilon\n\n    smoothed = TF.gaussian_blur(x_float, kernel_size=kernel_size_tuple, sigma=sigma_tuple)\n\n    # Bước 2: Chuẩn hóa noise_profile (sigmoid) và mở rộng cho đúng số kênh\n    # Sigmoid ensures blending weights are between 0 and 1\n    # A higher noise_profile value should lead to *more* smoothing.\n    # So, blending_weights = noise_profile (after sigmoid)\n    blending_weights = torch.sigmoid(noise_profile_float) # [B, 1, H, W]\n\n    # Expand blending_weights to match the number of channels in x\n    blending_weights = blending_weights.repeat(1, x_float.size(1), 1, 1) # [B, C, H, W]\n\n    # Ensure dimensions match for blending\n    assert blending_weights.shape == x_float.shape, f\"Blending weights shape {blending_weights.shape} does not match input shape {x_float.shape}\"\n\n    # Bước 3: Trộn ảnh gốc và ảnh đã làm mịn\n    # Output = (1 - alpha) * Original + alpha * Smoothed\n    # where alpha = blending_weights\n    weighted_sum = x_float * (1 - blending_weights) + smoothed * blending_weights\n\n    return weighted_sum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:18.115528Z","iopub.execute_input":"2025-08-04T10:04:18.115782Z","iopub.status.idle":"2025-08-04T10:04:22.846248Z","shell.execute_reply.started":"2025-08-04T10:04:18.115754Z","shell.execute_reply":"2025-08-04T10:04:22.845729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def quantum_noise_injection(features, T=1.25, pauli_prob={'X': 0.00096, 'Y': 0.00096, 'Z': 0.00096, 'None': 0.99712}):\n    \"\"\"\n    Áp dụng nhiễu lượng tử dựa trên cơ chế Pauli Noise Injection cho dữ liệu ảnh MRI.\n    \n    Args:\n        features (torch.Tensor): Tensor đầu vào dạng (batch_size, channels, height, width).\n        T (float): Hệ số nhiễu, thường trong khoảng [0.5, 1.5].\n        pauli_prob (dict): Phân phối xác suất cho các cổng Pauli (X, Y, Z, None).\n    \n    Returns:\n        torch.Tensor: Tensor đầu ra với nhiễu lượng tử được áp dụng.\n    \"\"\"\n    # Chuyển features sang kiểu float\n    features_float = features.float()\n    \n    # Kiểm tra kích thước tensor\n    if features_float.dim() < 4 or features_float.size(2) < 2 or features_float.size(3) < 2:\n        print(\"Warning: Features too small for quantum noise injection.\")\n        return features_float\n\n    try:\n        # Đảm bảo tensor ở trên thiết bị đúng\n        device = features_float.device\n        \n        # Chuẩn hóa xác suất Pauli với hệ số T\n        scaled_prob = {\n            'X': pauli_prob['X'] * T,\n            'Y': pauli_prob['Y'] * T,\n            'Z': pauli_prob['Z'] * T,\n            'None': 1.0 - (pauli_prob['X'] + pauli_prob['Y'] + pauli_prob['Z']) * T\n        }\n        \n        # Tạo mặt nạ ngẫu nhiên để chọn cổng Pauli\n        batch_size, channels, height, width = features_float.shape\n        pauli_choices = ['X', 'Y', 'Z', 'None']\n        probabilities = [scaled_prob['X'], scaled_prob['Y'], scaled_prob['Z'], scaled_prob['None']]\n        choice_tensor = torch.multinomial(\n            torch.tensor(probabilities, device=device),\n            batch_size * channels * height * width,\n            replacement=True\n        ).view(batch_size, channels, height, width)\n        \n        # Khởi tạo tensor đầu ra\n        noisy_features = features_float.clone()\n        \n        # Áp dụng cổng Pauli\n        for i, pauli in enumerate(pauli_choices):\n            mask = (choice_tensor == i)\n            if pauli == 'X':\n                # Cổng Pauli X: Lật giá trị pixel (giả sử giá trị đã chuẩn hóa trong [0, 1])\n                noisy_features[mask] = 1.0 - noisy_features[mask]\n            elif pauli == 'Y':\n                # Cổng Pauli Y: Kết hợp lật bit và thêm nhiễu ngẫu nhiên\n                noisy_features[mask] = 1.0 - noisy_features[mask] + 0.1 * torch.randn_like(noisy_features[mask], device=device)\n            elif pauli == 'Z':\n                # Cổng Pauli Z: Đổi dấu giá trị pixel\n                noisy_features[mask] = -noisy_features[mask]\n            # 'None': Giữ nguyên giá trị\n            \n        # Đảm bảo giá trị pixel nằm trong phạm vi [0, 1]\n        noisy_features = torch.clamp(noisy_features, 0.0, 1.0)\n        \n        return noisy_features\n    \n    except RuntimeError as e:\n        print(f\"Quantum noise injection failed: {e}. Returning original features.\")\n        return features_float","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:22.846993Z","iopub.execute_input":"2025-08-04T10:04:22.847305Z","iopub.status.idle":"2025-08-04T10:04:22.855660Z","shell.execute_reply.started":"2025-08-04T10:04:22.847286Z","shell.execute_reply":"2025-08-04T10:04:22.854954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model Components (U-Net based) ---\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv_block1 = BasicConvBlock(in_channels, out_channels)\n        self.conv_block2 = BasicConvBlock(out_channels, out_channels)\n        self.noise_estimator = ePURE(in_channels=in_channels)\n\n    def forward(self, x):\n        noise_profile = self.noise_estimator(x)\n        x_smoothed = adaptive_spline_smoothing(x, noise_profile)\n        x = self.conv_block1(x_smoothed)\n        x = self.conv_block2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:22.859513Z","iopub.execute_input":"2025-08-04T10:04:22.860002Z","iopub.status.idle":"2025-08-04T10:04:22.881706Z","shell.execute_reply.started":"2025-08-04T10:04:22.859971Z","shell.execute_reply":"2025-08-04T10:04:22.881015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MaxwellSolver(nn.Module):\n    def __init__(self, in_channels, hidden_dim=32):\n        super(MaxwellSolver, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1), nn.ReLU(),\n            nn.Conv2d(hidden_dim, 2, kernel_size=3, padding=1))\n        omega, mu_0, eps_0 = 2 * np.pi * 42.58e6, 4 * np.pi * 1e-7, 8.854187817e-12\n        self.k0 = torch.tensor(omega * np.sqrt(mu_0 * eps_0), dtype=torch.float32)\n\n    def forward(self, x):\n        eps_sigma_map = self.encoder(x)\n        return eps_sigma_map[:, 0:1, :, :], eps_sigma_map[:, 1:2, :, :]\n\n    def compute_helmholtz_residual(self, b1_map, eps, sigma):\n        self.k0 = self.k0.to(b1_map.device)\n        omega = 2 * np.pi * 42.58e6\n        b1_map_complex = torch.complex(b1_map, torch.zeros_like(b1_map)) if not b1_map.is_complex() else b1_map\n        eps_r, sig_r = eps.to(b1_map_complex.device), sigma.to(b1_map_complex.device)\n        size = b1_map_complex.shape[2:]\n        up_eps = F.interpolate(eps_r, size=size, mode='bilinear', align_corners=False)\n        up_sig = F.interpolate(sig_r, size=size, mode='bilinear', align_corners=False)\n        eps_c = torch.complex(up_eps, -up_sig / omega)\n        lap_b1 = self._laplacian_2d(b1_map_complex)\n        res = lap_b1 + (self.k0 ** 2) * eps_c * b1_map_complex\n        return res.real ** 2 + res.imag ** 2\n\n    def _laplacian_2d(self, x_complex):\n        k = torch.tensor([[0.,1.,0.],[1.,-4.,1.],[0.,1.,0.]], device=x_complex.device).reshape(1,1,3,3)\n        # Handle cases where real or imag part might have 0 channels if x_complex is purely real/imag\n        groups_real = x_complex.real.size(1) if x_complex.real.size(1) > 0 else 1\n        groups_imag = x_complex.imag.size(1) if x_complex.imag.size(1) > 0 else 1\n\n        real_lap = F.conv2d(x_complex.real, k.repeat(groups_real,1,1,1) if groups_real > 0 else k, padding=1, groups=groups_real)\n        imag_lap = F.conv2d(x_complex.imag, k.repeat(groups_imag,1,1,1) if groups_imag > 0 else k, padding=1, groups=groups_imag)\n        return torch.complex(real_lap, imag_lap)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:22.882288Z","iopub.execute_input":"2025-08-04T10:04:22.882476Z","iopub.status.idle":"2025-08-04T10:04:22.899872Z","shell.execute_reply.started":"2025-08-04T10:04:22.882460Z","shell.execute_reply":"2025-08-04T10:04:22.899315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n        concat_ch = in_channels // 2 + skip_channels\n        self.maxwell_solver = MaxwellSolver(concat_ch)\n        self.conv_block1 = BasicConvBlock(concat_ch, out_channels)\n        self.conv_block2 = BasicConvBlock(out_channels, out_channels)\n\n    def forward(self, x, skip_connection):\n        x = self.up(x)\n        diffY, diffX = skip_connection.size()[2]-x.size()[2], skip_connection.size()[3]-x.size()[3]\n        x = F.pad(x, [diffX//2, diffX-diffX//2, diffY//2, diffY-diffY//2])\n        x_cat = torch.cat([skip_connection, x], dim=1)\n        es_tuple = self.maxwell_solver(x_cat)\n        out = self.conv_block1(x_cat)\n        out = self.conv_block2(out)\n        return out, es_tuple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:22.900558Z","iopub.execute_input":"2025-08-04T10:04:22.900777Z","iopub.status.idle":"2025-08-04T10:04:22.912558Z","shell.execute_reply.started":"2025-08-04T10:04:22.900760Z","shell.execute_reply":"2025-08-04T10:04:22.912026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RobustMedVFL_UNet(nn.Module):\n    def __init__(self, n_channels=1, n_classes=4):\n        super().__init__()\n        self.enc1, self.pool1 = EncoderBlock(n_channels, 64), nn.MaxPool2d(2)\n        self.enc2, self.pool2 = EncoderBlock(64, 128), nn.MaxPool2d(2)\n        self.enc3, self.pool3 = EncoderBlock(128, 256), nn.MaxPool2d(2)\n        self.enc4, self.pool4 = EncoderBlock(256, 512), nn.MaxPool2d(2)\n        self.bottleneck = EncoderBlock(512, 1024)\n        self.dec1 = DecoderBlock(1024, 512, 512)\n        self.dec2 = DecoderBlock(512, 256, 256)\n        self.dec3 = DecoderBlock(256, 128, 128)\n        self.dec4 = DecoderBlock(128, 64, 64)\n        self.out_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n\n    def forward(self, x):\n        e1=self.enc1(x); p1=self.pool1(e1); e2=self.enc2(p1); p2=self.pool2(e2)\n        e3=self.enc3(p2); p3=self.pool3(e3); e4=self.enc4(p3); p4=self.pool4(e4)\n        b=self.bottleneck(p4)\n        d1,es1=self.dec1(b,e4); d2,es2=self.dec2(d1,e3)\n        d3,es3=self.dec3(d2,e2); d4,es4=self.dec4(d3,e1)\n        return self.out_conv(d4), (es1, es2, es3, es4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:22.913156Z","iopub.execute_input":"2025-08-04T10:04:22.913345Z","iopub.status.idle":"2025-08-04T10:04:22.926443Z","shell.execute_reply.started":"2025-08-04T10:04:22.913324Z","shell.execute_reply":"2025-08-04T10:04:22.925889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Any, Optional, Dict, List\n\nclass Adaptive_tvmf_dice_loss(nn.Module):\n    def __init__(\n            self,\n            num_classes: int = 4,\n            lambda_val: float = 15.0,\n            kappa_values=None,\n            epsilon: float = 1e-6):\n        super().__init__()\n        self.num_classes = num_classes\n        self.lambda_val = lambda_val\n        self.epsilon = epsilon\n        if kappa_values is not None:\n            self.register_buffer('kappa_values', torch.tensor(kappa_values, dtype=torch.float32))\n        else:\n            self.register_buffer('kappa_values', torch.ones(num_classes) * lambda_val)\n            \n    def update_kappa_values(self, new_kappa_values) -> None:\n        if isinstance(new_kappa_values, (list, np.ndarray)):\n            new_kappa_values = torch.tensor(new_kappa_values, dtype=torch.float32)\n        device = self.kappa_values.device\n        self.kappa_values.data = new_kappa_values.to(device)\n        \n    def t_vmf_similarity(self, cos_theta, kappa):\n        kappa = F.relu(kappa) + self.epsilon\n        return torch.exp(kappa * (cos_theta - 1))\n        \n    def compute_dice_coefficient(self, pred, target):\n        intersection = torch.sum(pred * target)\n        union = torch.sum(pred) + torch.sum(target)\n        dice = (2.0 * intersection + self.epsilon) / (union + self.epsilon)\n        return dice\n        \n    def forward(self, inputs, targets):\n        if inputs.dim() == 4:\n            inputs = F.softmax(inputs, dim=1)\n        else:\n            inputs = F.softmax(inputs, dim=-1)\n        if targets.dim() == 3:\n            targets_one_hot = F.one_hot(targets.long(), num_classes=self.num_classes)\n            targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()\n        else:\n            targets_one_hot = targets.float()\n        total_loss = 0.0\n        class_losses = []\n        for class_idx in range(self.num_classes):\n            pred_class = inputs[:, class_idx, :, :]\n            target_class = targets_one_hot[:, class_idx, :, :]\n            pred_flat = pred_class.contiguous().view(-1)\n            target_flat = target_class.contiguous().view(-1)\n            if torch.sum(target_flat) < self.epsilon:\n                class_losses.append(torch.tensor(0.0, device=inputs.device))\n                continue\n            cos_theta = F.cosine_similarity(pred_flat.unsqueeze(0), target_flat.unsqueeze(0), dim=1, eps=self.epsilon).squeeze()\n            kappa_tensor = getattr(self, 'kappa_values')\n            kappa = kappa_tensor[class_idx]\n            similarity = self.t_vmf_similarity(cos_theta, kappa)\n            dice_coeff = self.compute_dice_coefficient(pred_class, target_class)\n            tvmf_loss = 1.0 - similarity\n            dice_loss = 1.0 - dice_coeff\n            class_loss = tvmf_loss + dice_loss\n            class_losses.append(class_loss)\n            total_loss += class_loss\n        avg_loss = total_loss / self.num_classes\n        self.last_class_losses = torch.stack(class_losses)\n        return avg_loss\n        \n    def get_class_losses(self) -> Any:\n        if hasattr(self, 'last_class_losses'):\n            return self.last_class_losses.detach().cpu().numpy()\n        return np.zeros(self.num_classes)\n        \n    def get_adaptive_info(self) -> Any:\n        kappa_tensor = getattr(self, 'kappa_values')\n        return {'kappa_values': kappa_tensor.detach().cpu().numpy().tolist(), 'lambda_val': self.lambda_val, 'num_classes': self.num_classes}\n\n\nclass PhysicsLoss(nn.Module):\n    def __init__(self, in_channels_solver):\n        super().__init__()\n        self.ms = MaxwellSolver(in_channels_solver)\n    def forward(self, b1, eps, sig):\n        b, e, s = b1.to(DEVICE), eps.to(DEVICE), sig.to(DEVICE)\n        return torch.mean(self.ms.compute_helmholtz_residual(b, e, s))\n\n\nclass SmoothnessLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        dy = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :])\n        dx = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1])\n        return torch.mean(dy) + torch.mean(dx)\n\n\nclass AnatomicalRuleLoss(nn.Module):\n    \"\"\"\n    Tính toán loss dựa trên quy tắc giải phẫu về vị trí tương đối của các vùng tim.\n    - Phạt khi Tâm thất trái (LV) không được bao quanh bởi Cơ tim (MYO).\n    - Phạt khi Tâm thất phải (RV) nằm cạnh Cơ tim (MYO).\n    \"\"\"\n    def __init__(self, class_indices: Dict[str, int]):\n        \"\"\"\n        Args:\n            class_indices (Dict[str, int]): Dictionary ánh xạ tên class sang chỉ số.\n                                          Cần chứa các key: 'LV', 'MYO', 'RV'.\n        \"\"\"\n        super().__init__()\n        if not all(k in class_indices for k in ['LV', 'MYO', 'RV']):\n            raise ValueError(\"class_indices must contain keys 'LV', 'MYO', and 'RV'.\")\n        self.class_indices = class_indices\n\n    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            logits (torch.Tensor): Đầu ra raw từ model, shape (B, C, H, W).\n\n        Returns:\n            torch.Tensor: Giá trị loss vô hướng.\n        \"\"\"\n        pred_probs = torch.softmax(logits, dim=1)\n        \n        # Lấy bản đồ xác suất cho từng class\n        lv_prob = pred_probs[:, self.class_indices['LV']]\n        myo_prob = pred_probs[:, self.class_indices['MYO']]\n        rv_prob = pred_probs[:, self.class_indices['RV']]\n\n        # Mô phỏng phép giãn nở (dilation) bằng max_pool2d để tìm vùng lân cận\n        dilated_lv_prob = F.max_pool2d(lv_prob.unsqueeze(1), kernel_size=3, stride=1, padding=1).squeeze(1)\n        dilated_rv_prob = F.max_pool2d(rv_prob.unsqueeze(1), kernel_size=3, stride=1, padding=1).squeeze(1)\n\n        # Phạt 1: Vùng bao quanh LV (dilated_lv_prob) không phải là MYO\n        loss1 = dilated_lv_prob * (1 - myo_prob)\n\n        # Phạt 2: Vùng RV tiếp xúc với MYO\n        loss2 = dilated_rv_prob * myo_prob\n\n        # Kết hợp và lấy trung bình\n        total_rule_loss = torch.mean(loss1 + loss2)\n        return total_rule_loss\n        \n\nclass DynamicLossWeighter(nn.Module):\n    \"\"\"Dynamically adjusts weights for multiple loss components (e.g., CE, Dice, Physics).\"\"\"\n    def __init__(self, num_losses: int, tau: float = 1.0, initial_weights: Optional[List[float]] = None):\n        super().__init__()\n        self.num_losses = num_losses\n        self.tau = tau\n        if initial_weights:\n            assert len(initial_weights) == num_losses, \"Number of initial weights must be equal to num_losses\"\n            weights = torch.tensor(initial_weights, dtype=torch.float32)\n        else:\n            weights = torch.ones(num_losses, dtype=torch.float32)\n        self.log_vars = nn.Parameter(torch.log(weights))\n\n    def forward(self, individual_losses: torch.Tensor) -> torch.Tensor:\n        \"\"\"Calculates the total weighted loss.\"\"\"\n        if not isinstance(individual_losses, torch.Tensor):\n            individual_losses = torch.stack(individual_losses)\n        assert individual_losses.dim() == 1 and individual_losses.size(0) == self.num_losses, \\\n            f\"Input individual_losses must be a 1D tensor of size {self.num_losses}\"\n        total_loss = 0.0\n        for i in range(self.num_losses):\n            precision = torch.exp(-self.log_vars[i])\n            weighted_loss_term = precision * individual_losses[i] + self.log_vars[i]\n            total_loss += weighted_loss_term\n        return total_loss\n\n    def get_current_weights(self) -> Dict[str, float]:\n        \"\"\"Gets the current weights (calculated as exp(-log_var)) for monitoring.\"\"\"\n        with torch.no_grad():\n            weights = torch.exp(-self.log_vars)\n            return {f\"weight_{i}\": w.item() for i, w in enumerate(weights)}\n\n\nclass ClassWeightUpdater(nn.Module):\n    \"\"\"\n    Dynamically adjusts class weights for CrossEntropyLoss based on a combined\n    metric of class-wise Dice and IoU scores.\n    Uses an Exponential Moving Average (EMA) to stabilize weight updates.\n    \"\"\"\n    def __init__(self, num_classes: int, alpha: float = 0.9, epsilon: float = 1e-6):\n        \"\"\"\n        Args:\n            num_classes (int): Number of segmentation classes.\n            alpha (float): Smoothing factor for EMA. Higher alpha means slower updates.\n            epsilon (float): Small value to prevent division by zero.\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.alpha = alpha\n        self.epsilon = epsilon\n        # Buffer giờ sẽ lưu trữ điểm kết hợp (combined score) thay vì chỉ Dice\n        self.register_buffer('ema_combined_scores', torch.ones(num_classes))\n\n    def _calculate_per_class_metrics(self, logits: torch.Tensor, targets: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Calculates both Dice and IoU scores for each class.\n        \n        Returns:\n            A tuple containing (dice_scores, iou_scores) as tensors.\n        \"\"\"\n        probs = F.softmax(logits, dim=1)\n        targets_one_hot = F.one_hot(targets.long(), num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n        \n        dice_scores = []\n        iou_scores = []\n        \n        for i in range(self.num_classes):\n            pred_class = probs[:, i, :, :]\n            target_class = targets_one_hot[:, i, :, :]\n            \n            # Tính toán các thành phần cơ bản\n            intersection = torch.sum(pred_class * target_class)\n            pred_sum = torch.sum(pred_class)\n            target_sum = torch.sum(target_class)\n            \n            # Tính Dice Score\n            dice = (2. * intersection + self.epsilon) / (pred_sum + target_sum + self.epsilon)\n            dice_scores.append(dice)\n            \n            # Tính IoU Score (Jaccard Index)\n            union = pred_sum + target_sum - intersection\n            iou = (intersection + self.epsilon) / (union + self.epsilon)\n            iou_scores.append(iou)\n            \n        return torch.stack(dice_scores), torch.stack(iou_scores)\n\n    def update_and_get_weights(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Updates EMA and returns new class weights based on combined Dice and IoU performance.\n        Args:\n            logits (torch.Tensor): Raw output from the model (detached).\n            targets (torch.Tensor): Ground truth labels.\n        Returns:\n            torch.Tensor: New weights for CrossEntropyLoss.\n        \"\"\"\n        with torch.no_grad():\n            # 1. Tính toán cả hai chỉ số\n            current_dice, current_iou = self._calculate_per_class_metrics(logits, targets)\n            \n            # 2. Kết hợp điểm hiệu suất: lấy trung bình cộng\n            current_combined_score = (current_dice + 2 * current_iou) / 3.0\n            \n            # 3. Cập nhật EMA bằng điểm kết hợp\n            self.ema_combined_scores = self.alpha * self.ema_combined_scores + (1 - self.alpha) * current_combined_score\n            \n            # 4. Tính trọng số dựa trên nghịch đảo của điểm kết hợp đã được làm mượt\n            inverse_scores = 1.0 / (self.ema_combined_scores + self.epsilon)\n            \n            # 5. Chuẩn hóa trọng số\n            normalized_weights = self.num_classes * inverse_scores / torch.sum(inverse_scores)\n            \n            return normalized_weights\n\n\nclass CombinedLoss(nn.Module):\n    \"\"\"\n    Combined loss với hai cấp độ điều chỉnh trọng số động:\n    1. Trọng số động giữa các hàm loss khác nhau (CE, Dice, Physics, Smoothness, Anatomical).\n    2. Trọng số class động bên trong CrossEntropyLoss.\n    \"\"\"\n    def __init__(self, \n                 in_channels_maxwell=1024, \n                 num_classes=4, \n                 lambda_val=15.0, \n                 initial_loss_weights: Optional[List[float]] = None,\n                 # THÊM MỚI: Truyền vào class_indices để AnatomicalRuleLoss sử dụng\n                 class_indices_for_rules: Dict[str, int] = None):\n        super().__init__()\n        \n        # --- Initialize Class Weight Updater ---\n        self.class_weighter = ClassWeightUpdater(num_classes=num_classes) # Đã bỏ .to(DEVICE) để nhất quán\n        \n        # --- Initialize loss components ---\n        # 1. Cross Entropy Loss\n        self.ce = nn.CrossEntropyLoss()\n        \n        # 2. Adaptive t-vMF Dice Loss\n        self.dl = Adaptive_tvmf_dice_loss(num_classes=num_classes, lambda_val=lambda_val)\n        \n        # 3. Physics Loss\n        self.pl = PhysicsLoss(in_channels_maxwell)\n        \n        # 4. Smoothness Loss\n        self.sl = SmoothnessLoss()\n\n        # 5. THÊM MỚI: Anatomical Rule Loss\n        if class_indices_for_rules is None:\n            # Cung cấp giá trị mặc định hoặc báo lỗi nếu cần\n             raise ValueError(\"`class_indices_for_rules` must be provided for AnatomicalRuleLoss.\")\n        self.arl = AnatomicalRuleLoss(class_indices=class_indices_for_rules)\n        print(\"Initialized AnatomicalRuleLoss.\")\n        \n        # --- CẬP NHẬT: Initialize Loss Function Weighter cho 5 thành phần loss ---\n        # Số lượng loss giờ là 5\n        self.loss_weighter = DynamicLossWeighter(num_losses=5, initial_weights=initial_loss_weights)\n        \n        print(\"Initialized CombinedLoss with 5 components (CE, Dice, Physics, Smoothness, Anatomical).\")\n\n    def forward(self, logits, targets, b1=None, all_es=None, feat_sm=None):\n        \"\"\"Forward pass với hai cấp độ điều chỉnh trọng số động.\"\"\"\n        \n        # --- Step 1: Update and set dynamic class weights for CE ---\n        new_class_weights = self.class_weighter.update_and_get_weights(logits.detach(), targets)\n        self.ce.weight = new_class_weights.to(logits.device)\n        \n        # --- Step 2: Calculate individual loss components ---\n        lce = self.ce(logits, targets.long())\n        ldc = self.dl(logits, targets)\n\n        lphy = torch.tensor(0.0, device=logits.device)\n        if self.pl is not None and b1 is not None and all_es:\n            try:\n                e1, s1 = all_es[0]\n                lphy = self.pl(b1, e1, s1)\n            except (IndexError, TypeError):\n                print(\"Warning: Physics loss skipped due to unexpected `all_es` format.\")\n        \n        lsm = torch.tensor(0.0, device=logits.device)\n        if feat_sm is not None:\n            lsm = self.sl(feat_sm)\n\n        # THÊM MỚI: Tính anatomical rule loss\n        larl = self.arl(logits)\n\n        # --- Step 3: CẬP NHẬT: Kết hợp 5 thành phần loss ---\n        individual_losses = torch.stack([lce, ldc, lphy, lsm, larl])\n        total_loss = self.loss_weighter(individual_losses)\n\n        return total_loss\n\n    def get_current_loss_weights(self) -> Dict[str, float]:\n        \"\"\"Helper để theo dõi trọng số giữa các hàm loss.\"\"\"\n        weights = self.loss_weighter.get_current_weights()\n        # CẬP NHẬT: Thêm trọng số của loss mới\n        return {\n            \"weight_CE\": weights[\"weight_0\"],\n            \"weight_Dice\": weights[\"weight_1\"],\n            \"weight_Physics\": weights[\"weight_2\"],\n            \"weight_Smoothness\": weights[\"weight_3\"],\n            \"weight_Anatomical\": weights[\"weight_4\"],\n        }\n\n    def get_current_class_weights(self) -> Dict[str, float]:\n        \"\"\"Helper để theo dõi trọng số class động cho CrossEntropyLoss.\"\"\"\n        with torch.no_grad():\n            # Đảm bảo weight tồn tại và ở trên CPU để chuyển đổi\n            if self.ce.weight is not None:\n                current_weights = self.ce.weight.cpu()\n                return {f\"class_{i}_weight\": w.item() for i, w in enumerate(current_weights)}\n            return {} # Trả về rỗng nếu không có weight\n        \n    def get_kappa_values(self):\n        \"\"\"Get current κ values for monitoring\"\"\"\n        if isinstance(self.dl, Adaptive_tvmf_dice_loss):\n            return self.dl.get_adaptive_info()\n        return {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:22.927249Z","iopub.execute_input":"2025-08-04T10:04:22.927464Z","iopub.status.idle":"2025-08-04T10:04:22.961504Z","shell.execute_reply.started":"2025-08-04T10:04:22.927441Z","shell.execute_reply":"2025-08-04T10:04:22.960824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport nibabel as nib\nimport numpy as np\nfrom skimage.transform import resize\nimport sys\nimport configparser\n\n# --- Data Loading ---\ndef load_acdc_data(directory, is_training=True, target_size=(256, 256), max_patients=None):\n    imgs, msks = [], []\n    patient_count = 0\n\n    if not os.path.exists(directory):\n        print(f\"Error: Dataset directory not found at {directory}. \"\n              \"Please ensure the ACDC dataset is added to your Kaggle notebook inputs.\", file=sys.stderr)\n        return np.array([]), None\n\n    patient_folders = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n\n    for patient_folder in patient_folders:\n        if max_patients and patient_count >= max_patients:\n            break\n\n        patient_path = os.path.join(directory, patient_folder)\n        info_cfg_path = os.path.join(patient_path, 'Info.cfg')\n\n        ed_frame = -1\n        es_frame = -1\n        if os.path.exists(info_cfg_path):\n            parser = configparser.ConfigParser()\n            try:\n                with open(info_cfg_path, 'r') as f:\n                    config_string = '[DEFAULT]\\n' + f.read()\n                parser.read_string(config_string)\n\n                ed_frame = int(parser['DEFAULT']['ED'])\n                es_frame = int(parser['DEFAULT']['ES'])\n            except Exception as e:\n                print(f\"Warning: Could not parse Info.cfg for {patient_folder} in {directory}: {e}. Skipping patient.\", file=sys.stderr)\n                continue\n        else:\n            print(f\"Warning: Info.cfg not found for patient {patient_folder} in {directory}. Skipping patient.\", file=sys.stderr)\n            continue\n\n        ed_img_filename = f'{patient_folder}_frame{ed_frame:02d}.nii'\n        es_img_filename = f'{patient_folder}_frame{es_frame:02d}.nii'\n        ed_mask_filename = f'{patient_folder}_frame{ed_frame:02d}_gt.nii'\n        es_mask_filename = f'{patient_folder}_frame{es_frame:02d}_gt.nii'\n\n        ed_img_path = os.path.join(patient_path, ed_img_filename)\n        es_img_path = os.path.join(patient_path, es_img_filename)\n        ed_mask_path = os.path.join(patient_path, ed_mask_filename)\n        es_mask_path = os.path.join(patient_path, es_mask_filename)\n\n\n        def _load_nifti_and_process_slices(img_fpath, mask_fpath, target_sz, is_train_flag_for_warning):\n            current_images, current_masks = [], []\n            try:\n                if not os.path.exists(img_fpath):\n                    print(f\"Warning: Image file not found at {img_fpath}. Skipping this pair.\", file=sys.stderr)\n                    return None, None\n\n                img_nifti = nib.load(img_fpath)\n                img_data = img_nifti.get_fdata()\n\n                mask_data = None\n                # Always try to load mask if it exists\n                if os.path.exists(mask_fpath):\n                    mask_nifti = nib.load(mask_fpath)\n                    mask_data = mask_nifti.get_fdata()\n                elif is_train_flag_for_warning: # Only warn strongly if training set and mask is missing\n                    print(f\"Warning: Mask file not found for {img_fpath}. (Expected for training). Skipping this pair.\", file=sys.stderr)\n                    return None, None # Skip if mask is expected but not found\n\n                if img_data.ndim == 3: # (width, height, depth)\n                    for i in range(img_data.shape[2]):\n                        slice_img = img_data[:, :, i]\n                        resized_img = resize(\n                            slice_img, target_sz,\n                            order=1, preserve_range=True,\n                            anti_aliasing=True, mode='reflect'\n                        ).astype(np.float32)\n                        current_images.append(np.expand_dims(resized_img, axis=-1))\n\n                        if mask_data is not None:\n                            slice_mask = mask_data[:, :, i]\n                            resized_mask = resize(\n                                slice_mask, target_sz,\n                                order=0, preserve_range=True,\n                                anti_aliasing=False, mode='reflect'\n                            ).astype(np.uint8)\n                            current_masks.append(resized_mask)\n                elif img_data.ndim == 2: # Single 2D slice NIfTI file\n                     resized_img = resize(\n                        img_data, target_sz,\n                        order=1, preserve_range=True,\n                        anti_aliasing=True, mode='reflect'\n                    ).astype(np.float32)\n                     current_images.append(np.expand_dims(resized_img, axis=-1))\n\n                     if mask_data is not None:\n                         resized_mask = resize(\n                            mask_data, target_sz,\n                            order=0, preserve_range=True,\n                            anti_aliasing=False, mode='reflect'\n                        ).astype(np.uint8)\n                         current_masks.append(resized_mask)\n                else:\n                    print(f\"Warning: Unexpected image dimension ({img_data.ndim}) for {img_fpath}. Skipping.\", file=sys.stderr)\n                    return None, None\n\n            except Exception as e:\n                print(f\"Error processing {img_fpath}: {e}\", file=sys.stderr)\n                return None, None\n            return current_images, current_masks\n\n        # Load ED frame and its mask\n        ed_imgs, ed_msks = _load_nifti_and_process_slices(ed_img_path, ed_mask_path, target_size, is_training)\n        if ed_imgs:\n            imgs.extend(ed_imgs)\n            # Extend masks if they were loaded, regardless of is_training flag\n            if ed_msks: # Check if ed_msks is not None and not empty\n                msks.extend(ed_msks)\n\n        # Load ES frame and its mask\n        es_imgs, es_msks = _load_nifti_and_process_slices(es_img_path, es_mask_path, target_size, is_training)\n        if es_imgs:\n            imgs.extend(es_imgs)\n            # Extend masks if they were loaded, regardless of is_training flag\n            if es_msks: # Check if es_msks is not None and not empty\n                msks.extend(es_msks)\n\n        patient_count += 1\n\n    im_np = np.array(imgs, dtype=np.float32) if imgs else np.empty((0, target_size[0], target_size[1], 1), dtype=np.float32)\n    \n    # Only return msks_np if there are any masks collected\n    msk_np = np.array(msks, dtype=np.uint8) if msks else None\n\n    if msk_np is not None and msk_np.ndim == 4 and msk_np.shape[-1] == 1:\n        msk_np = np.squeeze(msk_np, axis=-1)\n\n    return im_np, msk_np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:22.962264Z","iopub.execute_input":"2025-08-04T10:04:22.962587Z","iopub.status.idle":"2025-08-04T10:04:23.708808Z","shell.execute_reply.started":"2025-08-04T10:04:22.962567Z","shell.execute_reply":"2025-08-04T10:04:23.708205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Metrics ---\ndef evaluate_metrics(model, dataloader, device, num_classes=4):\n    model.eval()\n    tp = [0] * num_classes\n    fp = [0] * num_classes\n    fn = [0] * num_classes\n    dice_s = [0.0] * num_classes\n    iou_s = [0.0] * num_classes\n    batches = 0\n\n    with torch.no_grad():\n        for imgs,tgts in dataloader:\n            imgs,tgts = imgs.to(device),tgts.to(device)\n            if imgs.size(0) == 0: continue\n            logits,_ = model(imgs)\n            preds = torch.argmax(F.softmax(logits,dim=1),dim=1); batches+=1\n            for c in range(num_classes):\n                pc_f,tc_f=(preds==c).float().view(-1),(tgts==c).float().view(-1); inter=(pc_f*tc_f).sum()\n                dice_s[c]+=((2.*inter+1e-6)/(pc_f.sum()+tc_f.sum()+1e-6)).item()\n                iou_s[c]+=((inter+1e-6)/(pc_f.sum()+tc_f.sum()-inter+1e-6)).item()\n                tp[c]+=inter.item(); fp[c]+=(pc_f.sum()-inter).item(); fn[c]+=(tc_f.sum()-inter).item()\n    metrics={'dice_scores':[],'iou':[],'precision':[],'recall':[],'f1_score':[]}\n    if batches>0:\n        for c in range(num_classes):\n            metrics['dice_scores'].append(dice_s[c]/batches); metrics['iou'].append(iou_s[c]/batches)\n            prec,rec = tp[c]/(tp[c]+fp[c]+1e-6), tp[c]/(tp[c]+fn[c]+1e-6)\n            metrics['precision'].append(prec); metrics['recall'].append(rec)\n            metrics['f1_score'].append(2*prec*rec/(prec+rec+1e-6) if (prec+rec > 0) else 0.0)\n    else: \n        for _ in range(num_classes): [metrics[key].append(0.0) for key in metrics]\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:23.709478Z","iopub.execute_input":"2025-08-04T10:04:23.709699Z","iopub.status.idle":"2025-08-04T10:04:23.718458Z","shell.execute_reply.started":"2025-08-04T10:04:23.709669Z","shell.execute_reply":"2025-08-04T10:04:23.717728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import Tuple, Optional, Union\nimport os\n\nclass B1MapCommonCalculator:\n    \"\"\"\n    Tính toán B1_map chung cho toàn bộ dataset ACDC\n    Kết hợp cả phương pháp simulation và học từ đặc trưng ảnh\n    \"\"\"\n    \n    def __init__(self, img_size: int = 256, device: str = 'cuda'):\n        self.img_size = img_size\n        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n        self.common_b1_map = None\n        self.dataset_statistics = {}\n        \n    def simulate_b1_map_physics_based(self, image_batch: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Mô phỏng B1_map dựa trên nguyên lý vật lý MRI và đặc trưng ảnh\n        \n        Args:\n            image_batch: Tensor shape (B, C, H, W)\n        Returns:\n            b1_maps: Tensor shape (B, 1, H, W)\n        \"\"\"\n        batch_size, channels, height, width = image_batch.shape\n        device = image_batch.device\n        \n        # Tạo coordinate grids\n        y_coords = torch.arange(height, dtype=torch.float32, device=device)\n        x_coords = torch.arange(width, dtype=torch.float32, device=device)\n        y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n        \n        center_y, center_x = height // 2, width // 2\n        \n        # Distance từ center (RF coil thường đặt ở giữa)\n        distance = torch.sqrt((x_grid - center_x)**2 + (y_grid - center_y)**2)\n        max_distance = torch.sqrt(torch.tensor(center_x**2 + center_y**2, device=device))\n        \n        b1_maps = []\n        \n        for b in range(batch_size):\n            # Lấy ảnh của batch hiện tại\n            current_image = image_batch[b, 0]  # Shape: (H, W)\n            \n            # 1. B1 inhomogeneity pattern cơ bản (giảm từ center ra ngoài)\n            b1_base = 1.0 - 0.25 * (distance / max_distance)\n            \n            # 2. Tissue-dependent variations\n            # Mô có cường độ cao (như máu) có dielectric constant khác\n            image_normalized = current_image / (torch.max(current_image) + 1e-8)\n            tissue_factor = 0.85 + 0.3 * image_normalized\n            \n            # 3. Cardiac-specific adjustments\n            # Tim có hình dạng và vị trí đặc biệt\n            cardiac_factor = self._get_cardiac_b1_pattern(current_image, height, width, device)\n            \n            # 4. RF coil loading effects\n            # Tải RF phụ thuộc vào phân bố mô\n            loading_effect = self._calculate_rf_loading(current_image, distance, device)\n            \n            # 5. Kết hợp các yếu tố\n            b1_map = b1_base * tissue_factor * cardiac_factor * loading_effect\n            \n            # 6. Thêm realistic noise và constraints\n            noise = torch.randn_like(b1_map, device=device) * 0.03\n            b1_map = b1_map + noise\n            \n            # Clip về range thực tế của B1 field (0.4 - 1.3)\n            b1_map = torch.clamp(b1_map, 0.4, 1.3)\n            \n            b1_maps.append(b1_map.unsqueeze(0))  # Add channel dimension\n        \n        return torch.stack(b1_maps, dim=0)  # Shape: (B, 1, H, W)\n    \n    def _get_cardiac_b1_pattern(self, image: torch.Tensor, height: int, width: int, device: torch.device) -> torch.Tensor:\n        \"\"\"\n        Tạo B1 pattern đặc biệt cho cardiac imaging\n        \"\"\"\n        # Cardiac region thường ở center-left của ảnh\n        cardiac_center_y = height // 2\n        cardiac_center_x = int(width * 0.4)  # Slightly left of center\n        \n        y_coords = torch.arange(height, dtype=torch.float32, device=device)\n        x_coords = torch.arange(width, dtype=torch.float32, device=device)\n        y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n        \n        # Distance từ cardiac center\n        cardiac_distance = torch.sqrt((x_grid - cardiac_center_x)**2 + (y_grid - cardiac_center_y)**2)\n        \n        # B1 field mạnh hơn ở cardiac region\n        cardiac_enhancement = 1.0 + 0.1 * torch.exp(-cardiac_distance / (width * 0.15))\n        \n        # Modulate bởi image intensity (cardiac structures có contrast cao)\n        image_weight = image / (torch.max(image) + 1e-8)\n        cardiac_factor = cardiac_enhancement * (0.9 + 0.2 * image_weight)\n        \n        return cardiac_factor\n    \n    def _calculate_rf_loading(self, image: torch.Tensor, distance: torch.Tensor, device: torch.device) -> torch.Tensor:\n        \"\"\"\n        Tính toán RF loading effect dựa trên phân bố mô\n        \"\"\"\n        # RF loading tăng khi có nhiều mô (high intensity regions)\n        tissue_density = image / (torch.max(image) + 1e-8)\n        \n        # Loading effect mạnh hơn ở center (gần RF coil)\n        loading_base = 1.0 - 0.1 * (distance / torch.max(distance))\n        \n        # Kết hợp với tissue density\n        loading_effect = loading_base * (0.95 + 0.1 * tissue_density)\n        \n        return loading_effect\n    \n    def calculate_dataset_common_b1_map(self, \n                                      all_images: torch.Tensor,\n                                      use_weighted_average: bool = True,\n                                      save_path: Optional[str] = None) -> torch.Tensor:\n        print(\"Calculating common B1 map for entire ACDC dataset...\")\n        \n        num_images = all_images.shape[0]\n        batch_size = min(16, num_images)  # Process in batches để tránh memory overflow\n        \n        all_b1_maps = []\n        image_statistics = []\n        \n        # Process images in batches\n        for i in range(0, num_images, batch_size):\n            end_idx = min(i + batch_size, num_images)\n            batch_images = all_images[i:end_idx].to(self.device)\n            \n            # Generate B1 maps for current batch\n            batch_b1_maps = self.simulate_b1_map_physics_based(batch_images)\n            all_b1_maps.append(batch_b1_maps.cpu())\n            \n            # Collect statistics\n            for j in range(batch_images.shape[0]):\n                img_stats = {\n                    'mean_intensity': torch.mean(batch_images[j]).item(),\n                    'std_intensity': torch.std(batch_images[j]).item(),\n                    'max_intensity': torch.max(batch_images[j]).item()\n                }\n                image_statistics.append(img_stats)\n            \n            if (i // batch_size + 1) % 10 == 0:\n                print(f\"Processed {i + batch_size}/{num_images} images...\")\n        \n        # Concatenate all B1 maps\n        all_b1_maps = torch.cat(all_b1_maps, dim=0)  # Shape: (N, 1, H, W)\n        \n        if use_weighted_average:\n            common_b1_map = self._calculate_weighted_average(all_b1_maps, image_statistics)\n        else:\n            common_b1_map = torch.mean(all_b1_maps, dim=0, keepdim=True)\n        \n        # Store results\n        self.common_b1_map = common_b1_map\n        self.dataset_statistics = {\n            'num_images': num_images,\n            'b1_range': (float(torch.min(common_b1_map)), float(torch.max(common_b1_map))),\n            'b1_mean': float(torch.mean(common_b1_map)),\n            'b1_std': float(torch.std(common_b1_map)),\n            'image_stats': {\n                'mean_intensity_avg': np.mean([s['mean_intensity'] for s in image_statistics]),\n                'std_intensity_avg': np.mean([s['std_intensity'] for s in image_statistics])\n            }\n        }\n        \n        print(f\"Common B1 map calculated successfully!\")\n        print(f\"  - B1 range: {self.dataset_statistics['b1_range'][0]:.3f} - {self.dataset_statistics['b1_range'][1]:.3f}\")\n        print(f\"  - B1 mean: {self.dataset_statistics['b1_mean']:.3f}\")\n        print(f\"  - B1 std: {self.dataset_statistics['b1_std']:.3f}\")\n        \n        # Save if path provided\n        if save_path:\n            self.save_common_b1_map(save_path)\n        \n        return common_b1_map\n    \n    def _calculate_weighted_average(self, all_b1_maps: torch.Tensor, image_stats: list) -> torch.Tensor:\n        \"\"\"\n        Tính weighted average, ưu tiên các vùng center và ảnh có contrast cao\n        \"\"\"\n        _, _, height, width = all_b1_maps.shape\n        \n        # Create spatial weights (higher weight for center regions)\n        y_coords = torch.arange(height, dtype=torch.float32)\n        x_coords = torch.arange(width, dtype=torch.float32)\n        y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n        \n        center_y, center_x = height // 2, width // 2\n        spatial_weights = torch.exp(-((x_grid - center_x)**2 + (y_grid - center_y)**2) / (2 * (min(height, width) / 4)**2))\n        spatial_weights = spatial_weights.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, H, W)\n        \n        # Create image-wise weights based on contrast\n        image_weights = []\n        for stats in image_stats:\n            # Higher weight for images with good contrast\n            contrast_score = stats['std_intensity'] / (stats['mean_intensity'] + 1e-8)\n            weight = min(max(contrast_score, 0.5), 2.0)  # Clip between 0.5 and 2.0\n            image_weights.append(weight)\n        \n        image_weights = torch.tensor(image_weights).view(-1, 1, 1, 1)  # Shape: (N, 1, 1, 1)\n        \n        # Apply weights\n        weighted_b1_maps = all_b1_maps * spatial_weights * image_weights\n        total_weights = spatial_weights * image_weights\n        \n        # Calculate weighted average\n        common_b1_map = torch.sum(weighted_b1_maps, dim=0, keepdim=True) / torch.sum(total_weights, dim=0, keepdim=True)\n        \n        return common_b1_map\n    \n    def get_b1_map_for_batch(self, batch_images: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Lấy B1_map cho một batch ảnh\n        Args:\n            batch_images: Tensor shape (B, C, H, W)\n        Returns:\n            b1_maps: Tensor shape (B, 1, H, W)\n        \"\"\"\n        if self.common_b1_map is not None:\n            # Sử dụng common B1 map, broadcast cho toàn batch\n            batch_size = batch_images.shape[0]\n            return self.common_b1_map.expand(batch_size, -1, -1, -1).to(batch_images.device)\n        else:\n            # Tính B1 map riêng cho batch này\n            return self.simulate_b1_map_physics_based(batch_images)\n    \n    def save_common_b1_map(self, save_path: str):\n        \"\"\"Lưu common B1 map và statistics\"\"\"\n        if self.common_b1_map is not None:\n            save_dict = {\n                'common_b1_map': self.common_b1_map,\n                'dataset_statistics': self.dataset_statistics,\n                'img_size': self.img_size\n            }\n            torch.save(save_dict, save_path)\n            # print(f\"Common B1 map saved to: {save_path}\")\n    \n    def load_common_b1_map(self, load_path: str):\n        \"\"\"Load common B1 map từ file đã lưu\"\"\"\n        if os.path.exists(load_path):\n            save_dict = torch.load(load_path, map_location=self.device)\n            self.common_b1_map = save_dict['common_b1_map']\n            self.dataset_statistics = save_dict['dataset_statistics']\n            self.img_size = save_dict.get('img_size', 256)\n            # print(f\"Common B1 map loaded from: {load_path}\")\n            # print(f\"  - B1 range: {self.dataset_statistics['b1_range'][0]:.3f} - {self.dataset_statistics['b1_range'][1]:.3f}\")\n        # else:\n            # print(f\"File not found: {load_path}\")\n\ndef integrate_b1_map_into_training(X_train_tensor: torch.Tensor, \n                                 X_val_tensor: torch.Tensor, \n                                 X_test_tensor: torch.Tensor,\n                                 img_size: int = 256,\n                                 device: str = 'cuda') -> B1MapCommonCalculator:\n    \n    # print(\"=== Integrating B1 Map Calculator ===\")\n    \n    # Initialize calculator\n    b1_calculator = B1MapCommonCalculator(img_size=img_size, device=device)\n    \n    # Try to load existing common B1 map\n    save_path = \"acdc_common_b1_map.pth\"\n    b1_calculator.load_common_b1_map(save_path)\n    \n    # If not loaded, calculate new one\n    if b1_calculator.common_b1_map is None:\n        # Combine all images for calculating common B1 map\n        all_images = torch.cat([X_train_tensor, X_val_tensor, X_test_tensor], dim=0)\n        \n        # Calculate common B1 map\n        common_b1_map = b1_calculator.calculate_dataset_common_b1_map(\n            all_images, \n            use_weighted_average=True,\n            save_path=save_path\n        )\n        \n        # print(f\"New common B1 map calculated and saved!\")\n    # else:\n    #     print(f\"Using existing common B1 map!\")\n    return b1_calculator\n\n# Hàm thay thế cho việc sử dụng trong training loop\ndef get_b1_map_for_training(images: torch.Tensor, \n                          b1_calculator: B1MapCommonCalculator) -> torch.Tensor:\n    return b1_calculator.get_b1_map_for_batch(images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:23.719312Z","iopub.execute_input":"2025-08-04T10:04:23.719577Z","iopub.status.idle":"2025-08-04T10:04:23.747023Z","shell.execute_reply.started":"2025-08-04T10:04:23.719554Z","shell.execute_reply":"2025-08-04T10:04:23.746383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom itertools import chain\n\n# --- Main Execution (Centralized Training) ---\nif __name__ == \"__main__\":\n    print(f\"Device: {DEVICE}\")\n\n    base_dataset_root = '/kaggle/input/automated-cardiac-diagnosis-challenge-miccai17/database'\n    \n    # Cập nhật đường dẫn chính xác cho thư mục training và testing\n    train_data_path = os.path.join(base_dataset_root, 'training') # Cập nhật lại đường dẫn này\n    test_data_path = os.path.join(base_dataset_root, 'testing')   # Cập nhật lại đường dẫn này\n\n    # Kiểm tra xem đường dẫn có tồn tại không\n    if not os.path.exists(train_data_path) or not os.listdir(train_data_path):\n        print(f\"Path '{train_data_path}' not found or empty. Using DUMMY data.\", file=sys.stderr)\n        # Tạo dữ liệu dummy nếu không tìm thấy dữ liệu thật\n        X_train_tensor = torch.randn(100, 1, IMG_SIZE, IMG_SIZE) # 100 mẫu huấn luyện\n        y_train_tensor = torch.randint(0, NUM_CLASSES, (100, IMG_SIZE, IMG_SIZE))\n        X_val_tensor = torch.randn(20, 1, IMG_SIZE, IMG_SIZE) # 20 mẫu validation\n        y_val_tensor = torch.randint(0, NUM_CLASSES, (20, IMG_SIZE, IMG_SIZE))\n        X_test_tensor = torch.randn(30, 1, IMG_SIZE, IMG_SIZE) # 30 mẫu test\n        y_test_tensor = torch.randint(0, NUM_CLASSES, (30, IMG_SIZE, IMG_SIZE)) # Dùng cho dummy, nếu không có mask thì bỏ qua\n\n    else:\n        # Tải toàn bộ dữ liệu huấn luyện\n        print(f\"Loading training data from: {train_data_path} (all patients)...\")\n        all_train_images_np, all_train_masks_np = load_acdc_data(\n            train_data_path, # Truyền đường dẫn đến thư mục 'training'\n            is_training=True,\n            target_size=(IMG_SIZE, IMG_SIZE),\n            max_patients=None # Load tất cả 100 bệnh nhân\n        )\n        print(f\"Loaded {all_train_images_np.shape[0]} training images.\")\n        print(f\"Shape of training images: {all_train_images_np.shape}\")\n        if all_train_masks_np is not None:\n            print(f\"Shape of training masks: {all_train_masks_np.shape}\")\n        else:\n            raise ValueError(\"Training masks are None. They are required for training.\")\n\n        # Tải toàn bộ dữ liệu kiểm tra\n        print(f\"\\nLoading testing data from: {test_data_path} (all patients)...\")\n        all_test_images_np, all_test_masks_np = load_acdc_data( # Nên giữ all_test_masks_np để kiểm tra\n            test_data_path, \n            is_training=False, # Mask không bắt buộc cho tập kiểm tra\n            target_size=(IMG_SIZE, IMG_SIZE),\n            max_patients=None # Load tất cả 50 bệnh nhân\n        )\n        print(f\"Loaded {all_test_images_np.shape[0]} testing images.\")\n        print(f\"Shape of testing images: {all_test_images_np.shape}\")\n        if all_test_masks_np is None:\n            print(\"No masks loaded for testing set (as expected if not present in source).\")\n        else:\n            print(f\"Shape of testing masks (if loaded): {all_test_masks_np.shape}\")\n\n        if all_train_images_np.size == 0:\n            raise ValueError(\"Training data is empty after loading. Check data path and content.\")\n            \n        if all_test_images_np.size == 0:\n            print(\"Warning: Test data is empty after loading. Evaluation might be affected.\", file=sys.stderr)\n\n        # Normalize ảnh về khoảng [0, 1]\n        # Chú ý: Cần xử lý giá trị NaN/inf nếu có do max = 0\n        if np.max(all_train_images_np) > 0:\n            all_train_images_np = all_train_images_np / np.max(all_train_images_np)\n        else:\n            print(\"Warning: Max value of training images is 0, no normalization applied.\", file=sys.stderr)\n        \n        if np.max(all_test_images_np) > 0:\n            all_test_images_np = all_test_images_np / np.max(all_test_images_np)\n        else:\n            print(\"Warning: Max value of testing images is 0, no normalization applied.\", file=sys.stderr)\n\n        # Chuyển đổi dữ liệu kiểm tra sang Tensor và tạo DataLoader\n        X_test_tensor = torch.tensor(all_test_images_np).permute(0, 3, 1, 2).float()\n\n        # Xử lý y_test_tensor: chỉ tạo nếu mask có sẵn, nếu không thì dùng nhãn giả hoặc không dùng\n        if all_test_masks_np is not None:\n            y_test_tensor = torch.tensor(all_test_masks_np).long()\n            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n            print(\"Test DataLoader will include masks.\")\n        else:\n            # Nếu không có mask, tạo DataLoader chỉ với ảnh\n            test_dataset = TensorDataset(X_test_tensor) # Chỉ có ảnh\n            print(\"Test DataLoader will NOT include masks (as they are not available for testing set).\")\n        \n        test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True if DEVICE.type == 'cuda' else False)\n        print(f\"Test samples: {len(test_dataset)}\")\n\n        # Chia train/validation từ toàn bộ dữ liệu huấn luyện đã tải\n        # Đảm bảo cả X và y đều không rỗng trước khi chia\n        if all_train_images_np.shape[0] > 0 and all_train_masks_np.shape[0] > 0:\n            X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n                all_train_images_np, all_train_masks_np, test_size=0.2, random_state=42 # 20% cho validation\n            )\n        else:\n            raise ValueError(\"Not enough training data to perform train/val split.\")\n\n        # Chuyển đổi dữ liệu huấn luyện và validation sang Tensor và tạo DataLoader\n        X_train_tensor = torch.tensor(X_train_np).permute(0, 3, 1, 2).float()\n        y_train_tensor = torch.tensor(y_train_np).long()\n        X_val_tensor = torch.tensor(X_val_np).permute(0, 3, 1, 2).float()\n        y_val_tensor = torch.tensor(y_val_np).long()\n\n    # Kiểm tra kích thước của các tập dữ liệu sau khi chia\n    if len(X_train_tensor) == 0: raise ValueError(\"No training samples after split.\")\n    if len(X_val_tensor) == 0: print(\"Warning: Validation set is empty after split.\", file=sys.stderr)\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n\n    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True if DEVICE.type == 'cuda' else False)\n    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True if DEVICE.type == 'cuda' else False)\n\n    print(f\"\\nTraining samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n    print(\"Data loaded and prepared for centralized training.\")\n\n    # --- Initialize Model, Criterion, Optimizer ---\n    # Đảm bảo các class này đã được import hoặc định nghĩa\n    model = RobustMedVFL_UNet(n_channels=1, n_classes=NUM_CLASSES).to(DEVICE)\n    # criterion = CombinedLoss(num_classes=NUM_CLASSES, in_channels_maxwell=1024).to(DEVICE)\n    my_class_indices = {'RV': 1, 'MYO': 2, 'LV': 3}\n\n    # Khởi tạo loss function\n    criterion = CombinedLoss(\n        in_channels_maxwell=1024,\n        num_classes=4,\n        lambda_val=15.0,\n        initial_loss_weights=[0.3, 0.5, 0.5, 1.0, 0.5], # Khởi tạo trọng số cho 5 loss\n        class_indices_for_rules=my_class_indices\n    ).to(DEVICE)\n    optimizer = torch.optim.Adam(\n    chain(model.parameters(), criterion.parameters()), \n    lr=LEARNING_RATE\n    )\n    \n    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5) # Tùy chọn\n\n    # --- Centralized Training Loop ---\n    best_val_metric = 0.0 # Hoặc float('inf') nếu loss là metric chính\n\n    for epoch in range(NUM_EPOCHS_CENTRALIZED):\n        print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS_CENTRALIZED} ---\")\n        \n        # Training phase\n        model.train()\n        epoch_train_loss = 0.0\n        num_train_batches = 0\n        \n        for images, targets in train_dataloader:\n            images, targets = images.to(DEVICE), targets.to(DEVICE)\n            \n            images_noisy = quantum_noise_injection(images) # Tùy chọn áp dụng noise\n            \n            optimizer.zero_grad()\n            logits, all_eps_sigma_tuples = model(images_noisy)\n\n            b1_calculator = integrate_b1_map_into_training(\n            X_train_tensor, X_val_tensor, X_test_tensor,\n            img_size=IMG_SIZE, device=DEVICE\n            )\n            b1_map = get_b1_map_for_training(images, b1_calculator)\n            loss = criterion(logits, targets, b1_map, all_eps_sigma_tuples) #, features_for_smoothness=None)\n            \n            loss.backward()\n            optimizer.step()\n            \n            epoch_train_loss += loss.item()\n            num_train_batches += 1\n            \n        avg_train_loss = epoch_train_loss / num_train_batches if num_train_batches > 0 else 0\n        print(f\"   Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n        \n        # Validation phase\n        if val_dataloader.dataset and len(val_dataloader.dataset) > 0:\n            print(\"   Evaluating on validation set...\")\n            val_metrics = evaluate_metrics(model, val_dataloader, DEVICE, NUM_CLASSES)\n            # Sử dụng Dice score của class foreground trung bình làm metric chính để so sánh\n            # Lấy các giá trị foreground (class từ 1 trở đi)\n            fg_dice = val_metrics['dice_scores'][1:] if NUM_CLASSES > 1 else [val_metrics['dice_scores'][0]]\n            fg_iou = val_metrics['iou'][1:] if NUM_CLASSES > 1 else [val_metrics['iou'][0]]\n            fg_precision = val_metrics['precision'][1:] if NUM_CLASSES > 1 else [val_metrics['precision'][0]]\n            fg_recall = val_metrics['recall'][1:] if NUM_CLASSES > 1 else [val_metrics['recall'][0]]\n            fg_f1 = val_metrics['f1_score'][1:] if NUM_CLASSES > 1 else [val_metrics['f1_score'][0]]\n            \n            avg_fg_dice = np.mean(fg_dice)\n            avg_fg_iou = np.mean(fg_iou)\n            avg_fg_precision = np.mean(fg_precision)\n            avg_fg_recall = np.mean(fg_recall)\n            avg_fg_f1 = np.mean(fg_f1)\n            \n            print(f\"   Epoch {epoch+1} - Validation (Avg Foreground): \"\n                  f\"Dice: {avg_fg_dice:.4f}; IoU: {avg_fg_iou:.4f}; \"\n                  f\"Precision: {avg_fg_precision:.4f}; Recall: {avg_fg_recall:.4f}; F1-score: {avg_fg_f1:.4f}\")\n            for c_idx in range(NUM_CLASSES):\n                print(f\"     Class {c_idx}: Dice: {val_metrics['dice_scores'][c_idx]:.4f}; \"\n                      f\"IoU: {val_metrics['iou'][c_idx]:.4f}; \"\n                      f\"Precision: {val_metrics['precision'][c_idx]:.4f}; \"\n                      f\"Recall: {val_metrics['recall'][c_idx]:.4f}; \"\n                      f\"F1-score: {val_metrics['f1_score'][c_idx]:.4f}\")\n\n            # Tùy chọn: Lưu model tốt nhất dựa trên val_metric\n            if avg_fg_dice > best_val_metric:\n                best_val_metric = avg_fg_dice\n                # torch.save(model.state_dict(), \"best_centralized_model.pth\")\n                # print(f\"     New best model saved with Val Dice: {best_val_metric:.4f}\")\n            \n            # if scheduler: scheduler.step(avg_val_loss_or_metric) # Nếu dùng scheduler\n        else:\n            print(\"   Validation dataset is empty. Skipping validation.\")\n\n        current_loss_weights = criterion.get_current_loss_weights() # Giả sử object loss của bạn tên là `criterion`\n        current_class_weights = criterion.get_current_class_weights()\n    \n        print(f\"\\n   Epoch {epoch+1} - Learned Loss Weights:\")\n        for name, weight in current_loss_weights.items():\n            print(f\"     - {name}: {weight:.4f}\")\n        \n        print(f\"   Epoch {epoch+1} - Dynamic Class Weights (for CE):\")\n        class_weights_str = \" | \".join([f\"Class {i}: {w:.4f}\" for i, w in enumerate(current_class_weights.values())])\n        print(f\"     - {class_weights_str}\")\n        \n        print(\"-\" * 60)\n\n    print(\"\\n--- Centralized Training Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:04:23.747761Z","iopub.execute_input":"2025-08-04T10:04:23.747949Z","iopub.status.idle":"2025-08-04T10:50:58.129182Z","shell.execute_reply.started":"2025-08-04T10:04:23.747935Z","shell.execute_reply":"2025-08-04T10:50:58.128453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Trực quan kết quả sau huấn luyện","metadata":{}},{"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n# Các định nghĩa cho việc trực quan hóa (giữ nguyên)\nACDC_CLASS_MAP = {\n    0: \"Background\",\n    1: \"Right Ventricle\",\n    2: \"Myocardium\",\n    3: \"Left Ventricle\"\n}\nACDC_COLOR_MAP = {\n    0: 'black',\n    1: '#FF0000',\n    2: '#00FF00',\n    3: '#0000FF'\n}\n\ndef visualize_final_results(model, images_np, masks_np, num_classes, num_samples=3, device=None):\n    \"\"\"\n    Trực quan hóa và so sánh kết quả của mô hình trên các ảnh ngẫu nhiên.\n\n    Args:\n        model (torch.nn.Module): Mô hình PyTorch đã được huấn luyện.\n        images_np (np.ndarray): Mảng numpy chứa các ảnh.\n        masks_np (np.ndarray): Mảng numpy chứa các mặt nạ ground truth.\n        num_classes (int): Số lượng lớp của bài toán segmentation. <--- THAM SỐ MỚI\n        num_samples (int): Số lượng mẫu ngẫu nhiên để hiển thị.\n        device (torch.device, optional): Thiết bị để chạy mô hình.\n    \"\"\"\n    if not device:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model.to(device)\n    model.eval()\n\n    total_images = images_np.shape[0]\n    if total_images == 0:\n        print(\"Không có ảnh nào để trực quan hóa.\")\n        return\n        \n    sample_indices = random.sample(range(total_images), min(num_samples, total_images))\n\n    for idx in sample_indices:\n        image_np_single = images_np[idx]\n        image_tensor = torch.from_numpy(image_np_single).permute(2, 0, 1).unsqueeze(0)\n        image_tensor = image_tensor.to(device, dtype=torch.float32)\n\n        with torch.no_grad():\n            output, _ = model(image_tensor)\n            pred_mask_tensor = torch.argmax(output, dim=1)\n        \n        pred_mask_np = pred_mask_tensor.cpu().squeeze().numpy()\n\n        has_gt_mask = masks_np is not None and idx < len(masks_np)\n        num_subplots = 3 if has_gt_mask else 2\n        fig, axes = plt.subplots(1, num_subplots, figsize=(13 * num_subplots / 2, 7))\n        fig.suptitle(f'Kết quả cho ảnh số {idx}', fontsize=16)\n        \n        # SỬA LỖI Ở ĐÂY: Dùng `num_classes` thay vì `model.n_classes`\n        colors = [ACDC_COLOR_MAP.get(i, 'black') for i in range(num_classes)]\n        cmap = mcolors.ListedColormap(colors)\n        \n        axes[0].imshow(image_np_single.squeeze(), cmap='gray')\n        axes[0].set_title('Ảnh MRI Gốc')\n        axes[0].axis('off')\n\n        ax_pred = axes[1]\n        ax_pred.imshow(image_np_single.squeeze(), cmap='gray')\n        pred_masked_display = np.ma.masked_where(pred_mask_np == 0, pred_mask_np)\n        ax_pred.imshow(pred_masked_display, cmap=cmap, alpha=0.6, vmin=0, vmax=len(colors)-1)\n        ax_pred.set_title('Dự đoán của mô hình')\n        ax_pred.axis('off')\n\n        if has_gt_mask:\n            gt_mask_np = masks_np[idx]\n            ax_gt = axes[2]\n            ax_gt.imshow(image_np_single.squeeze(), cmap='gray')\n            gt_masked_display = np.ma.masked_where(gt_mask_np == 0, gt_mask_np)\n            ax_gt.imshow(gt_masked_display, cmap=cmap, alpha=0.6, vmin=0, vmax=len(colors)-1)\n            ax_gt.set_title('Mặt nạ Ground Truth')\n            ax_gt.axis('off')\n\n        legend_elements = [\n            plt.Rectangle((0, 0), 1, 1, color=ACDC_COLOR_MAP[i], label=ACDC_CLASS_MAP[i])\n            for i in range(1, num_classes)\n        ]\n        fig.legend(handles=legend_elements, loc='lower center', ncol=3, bbox_to_anchor=(0.5, 0.02))\n\n        plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n        plt.show()\n\n\n# --- TRỰC QUAN HÓA KẾT QUẢ CUỐI CÙNG ---\nprint(\"\\n--- Visualizing Final Model Predictions on Test Set ---\")\n\n# Gọi hàm trực quan hóa\n# Hàm này sẽ sử dụng model sau khi đã huấn luyện xong và bộ dữ liệu test đã tải\n# all_test_images_np và all_test_masks_np đã được định nghĩa ở đầu mã của bạn\nvisualize_final_results(\n    model=model,\n    images_np=all_test_images_np,\n    masks_np=all_test_masks_np, # Truyền cả mặt nạ (nếu có)\n    num_classes=NUM_CLASSES,  # <--- THÊM DÒNG NÀY\n    num_samples=3,              # Số lượng ảnh ngẫu nhiên muốn xem\n    device=DEVICE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:50:58.130543Z","iopub.execute_input":"2025-08-04T10:50:58.131094Z","iopub.status.idle":"2025-08-04T10:51:00.239093Z","shell.execute_reply.started":"2025-08-04T10:50:58.131065Z","shell.execute_reply":"2025-08-04T10:51:00.238493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Trực quan kết quả trên tập test set\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef run_and_print_test_evaluation(model, test_dataloader, device, num_classes):\n    \"\"\"\n    Args:\n        model (torch.nn.Module): Mô hình PyTorch đã được huấn luyện.\n        test_dataloader (torch.utils.data.DataLoader): DataLoader cho tập test.\n        device (torch.device): Thiết bị để chạy mô hình ('cuda' hoặc 'cpu').\n        num_classes (int): Số lượng lớp của bài toán.\n    \"\"\"\n    # Kiểm tra xem test_dataloader có hợp lệ không\n    if test_dataloader and test_dataloader.dataset and len(test_dataloader.dataset) > 0:\n        print(\"\\n--- Evaluating on Test Set ---\")\n        \n        # Gọi hàm tính toán metrics (hàm này cần được định nghĩa ở nơi khác)\n        test_metrics = evaluate_metrics(model, test_dataloader, device, num_classes)\n\n        # Lấy các chỉ số cho các lớp foreground (từ lớp 1 trở đi)\n        if num_classes > 1:\n            fg_dice = test_metrics['dice_scores'][1:]\n            fg_iou = test_metrics['iou'][1:]\n            fg_precision = test_metrics['precision'][1:]\n            fg_recall = test_metrics['recall'][1:]\n            fg_f1 = test_metrics['f1_score'][1:]\n        else: # Trường hợp chỉ có 1 lớp\n            fg_dice = [test_metrics['dice_scores'][0]]\n            fg_iou = [test_metrics['iou'][0]]\n            fg_precision = [test_metrics['precision'][0]]\n            fg_recall = [test_metrics['recall'][0]]\n            fg_f1 = [test_metrics['f1_score'][0]]\n        \n        # In kết quả trung bình của các lớp foreground\n        print(f\"  Test (Avg Foreground): \"\n              f\"Dice: {np.mean(fg_dice):.4f}; IoU: {np.mean(fg_iou):.4f}; \"\n              f\"Precision: {np.mean(fg_precision):.4f}; Recall: {np.mean(fg_recall):.4f}; \"\n              f\"F1-score: {np.mean(fg_f1):.4f}\")\n        \n        # In kết quả chi tiết cho từng lớp\n        for c_idx in range(num_classes):\n            print(f\"    Class {c_idx} ({ACDC_CLASS_MAP.get(c_idx, 'N/A')}): \"\n                  f\"Dice: {test_metrics['dice_scores'][c_idx]:.4f}; \"\n                  f\"IoU: {test_metrics['iou'][c_idx]:.4f}; \"\n                  f\"Precision: {test_metrics['precision'][c_idx]:.4f}; \"\n                  f\"Recall: {test_metrics['recall'][c_idx]:.4f}; \"\n                  f\"F1-score: {test_metrics['f1_score'][c_idx]:.4f}\")\n    else:\n        print(\"\\nTest dataset not available or empty. Skipping test evaluation.\")\n\n# --- Evaluate on Test Set ---\n\n# 1. Chạy đánh giá và in các chỉ số metrics\nrun_and_print_test_evaluation(\n    model=model,\n    test_dataloader=test_dataloader,\n    device=DEVICE,\n    num_classes=NUM_CLASSES\n)\n\n# 2. Trực quan hóa kết quả trên một vài ảnh ngẫu nhiên từ tập test\nprint(\"\\n--- Visualizing Predictions on Test Set Samples ---\")\n# Sử dụng các biến numpy đã tải từ trước (all_test_images_np, all_test_masks_np)\nvisualize_final_results(\n    model=model,\n    images_np=all_test_images_np,\n    masks_np=all_test_masks_np,\n    num_classes=NUM_CLASSES,\n    num_samples=50, # Số lượng ảnh muốn xem\n    device=DEVICE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T10:51:00.239932Z","iopub.execute_input":"2025-08-04T10:51:00.240164Z","iopub.status.idle":"2025-08-04T10:51:41.908018Z","shell.execute_reply.started":"2025-08-04T10:51:00.240145Z","shell.execute_reply":"2025-08-04T10:51:41.907210Z"}},"outputs":[],"execution_count":null}]}